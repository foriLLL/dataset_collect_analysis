{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "import os\n",
    "from pathlib import Path\n",
    "script_dir = get_ipython().starting_dir\n",
    "# 修改工作目录为上一级\n",
    "os.chdir(Path(script_dir) / '..')\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from util.conflict_util import Conflict, conflict2file\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import re\n",
    "work_dir = Path(os.getcwd())\n",
    "print(work_dir)\n",
    "\n",
    "class ConflictChunk:\n",
    "    def __init__(self, m_start, m_end, a_content, b_content, \n",
    "                 o_content, r_content, label: str | None, chunk_idx):\n",
    "        self.m_start = m_start\n",
    "        self.m_end = m_end\n",
    "        self.a_content: 'str' = a_content\n",
    "        self.b_content: 'str' = b_content\n",
    "        self.o_content: 'str' = o_content\n",
    "        self.r_content: 'str' = r_content\n",
    "        self.label = label\n",
    "        self.chunk_idx = chunk_idx\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"m_start\": self.m_start,\n",
    "            \"m_end\": self.m_end,\n",
    "            \"a_content\": self.a_content,\n",
    "            \"b_content\": self.b_content,\n",
    "            \"o_content\": self.o_content,\n",
    "            \"r_content\": self.r_content,\n",
    "            \"label\": self.label,\n",
    "        }\n",
    "    \n",
    "    def getJSONstr(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, indent=4)\n",
    "\n",
    "\n",
    "class ConflictFile:\n",
    "    def __init__(self, path, repo_url, file_a_content, file_b_content, file_o_content, file_r_content, file_m_content, commit_hash):\n",
    "        self.path = path\n",
    "        self.repo_url = repo_url\n",
    "        self.file_a_content = file_a_content\n",
    "        self.file_b_content = file_b_content\n",
    "        self.file_o_content = file_o_content\n",
    "        self.file_r_content = file_r_content\n",
    "        self.file_m_content = file_m_content\n",
    "        self.commit_hash = commit_hash\n",
    "        self.conflict_chunks = []\n",
    "\n",
    "    def add_conflict_chunk(self, conflict_chunk_obj):\n",
    "        self.conflict_chunks.append(conflict_chunk_obj)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"path\": self.path,\n",
    "            \"repo_url\": self.repo_url,\n",
    "            \"file_a_content\": self.file_a_content,\n",
    "            \"file_b_content\": self.file_b_content,\n",
    "            \"file_o_content\": self.file_o_content,\n",
    "            \"file_r_content\": self.file_r_content,\n",
    "            \"file_m_content\": self.file_m_content,\n",
    "            \"conflict_chunks\": [chunk.to_dict() for chunk in self.conflict_chunks],\n",
    "        }\n",
    "    \n",
    "    def getJSONstr(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, indent=4)\n",
    "    \n",
    "class ConflictFileCollector:\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "    \n",
    "    @staticmethod\n",
    "    def sample(output_dir, n, random_seed=0, label=None):\n",
    "        cnt = 0\n",
    "        # 从所有冲突文件中随机抽取 n 个 label 类型的 Conflict chunk\n",
    "        # 读取 output_dir 中的所有 JSON 文件\n",
    "        jsons = list(ConflictFileCollector.getAllJsonsUnder(output_dir))\n",
    "        print(f\"Found {len(jsons)} JSON files in {output_dir}\")\n",
    "        # 读取所有 JSON 文件中的 Conflict chunk\n",
    "        for json_file in jsons:\n",
    "            with open(json_file) as f:\n",
    "                data = json.load(f)\n",
    "            for conflict_file in data:\n",
    "                for chunk in conflict_file['conflict_chunks']:\n",
    "                    if label == None or chunk['label'] == label:\n",
    "                        if cnt >= n:\n",
    "                            return\n",
    "                        cnt += 1\n",
    "                        yield chunk\n",
    "\n",
    "\n",
    "    def collect(self):\n",
    "        '''\n",
    "        返回一个迭代器，每次迭代返回一个ConflictFile对象\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def collect_in_batches(self, batch_size=10000):\n",
    "        batch = []\n",
    "        for conflict_file in self.collect():\n",
    "            if conflict_file is None:\n",
    "                continue\n",
    "            batch.append(conflict_file)\n",
    "            if len(batch) >= batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            yield batch\n",
    "\n",
    "    def collect_and_save(self, output_dir, batch_size=10000):\n",
    "        output_dir = Path(output_dir)  # 确保 output_dir 是 Path 对象\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)  # 自动创建目录及其父目录\n",
    "        for i, batch in enumerate(self.collect_in_batches(batch_size)):\n",
    "            with open(output_dir / f\"{i}.json\", 'w') as f:\n",
    "                print(f\"Saving batch {i} to {output_dir / f'{i}.json'}\")\n",
    "                json.dump([json.loads(x.getJSONstr()) for x in batch], f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocessContent(content: str):\n",
    "        return '' if content.strip() == '' else re.sub(r'\\s+', ' ', content.strip() + '\\n')\n",
    "    \n",
    "    @staticmethod\n",
    "    def getLabel(a: str, b: str, o: str, r: str):\n",
    "        r_processed = ConflictFileCollector.preprocessContent(r)\n",
    "        a_processed = ConflictFileCollector.preprocessContent(a)\n",
    "        b_processed = ConflictFileCollector.preprocessContent(b)\n",
    "        o_processed = ConflictFileCollector.preprocessContent(o)\n",
    "        if a_processed == b_processed:\n",
    "            return \"same modification, formatting maybe different\"\n",
    "        if r_processed == a_processed:\n",
    "            return \"A\"\n",
    "        if r_processed == b_processed:\n",
    "            return \"B\"\n",
    "        if r_processed == o_processed:\n",
    "            return \"O\"\n",
    "        if r_processed == a_processed + b_processed:\n",
    "            return \"AB\"\n",
    "        if r_processed == b_processed + a_processed:\n",
    "            return \"BA\"\n",
    "\n",
    "        r_lines = set(r.split('\\n'))\n",
    "        a_lines = set(a.split('\\n'))\n",
    "        b_lines = set(b.split('\\n'))\n",
    "        o_lines = set(o.split('\\n'))\n",
    "        for rl in r_lines:\n",
    "            if (rl not in a_lines) and (rl not in b_lines) and (rl not in o_lines) and not rl.isspace():\n",
    "                return 'newline'\n",
    "        return 'mixline'\n",
    "\n",
    "    @staticmethod\n",
    "    def getAllJsonsUnder(dirPath: str):\n",
    "        for root, _, files in os.walk(dirPath):\n",
    "            for file in files:\n",
    "                if(file.endswith(\".json\")):\n",
    "                    yield os.path.join(root, file)\n",
    "    \n",
    "    @staticmethod\n",
    "    def list2str(l):\n",
    "        if l == [] or l == ['']:\n",
    "            return ''\n",
    "        return '\\n'.join(l) + '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 观察样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(ConflictFileCollector.sample(work_dir / 'data_collect_analysis' / 'output' / '100+stars_4GB-_multidev_org_lang', n=10, label='mixline'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 观察不能被 es 解决的 mixline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks4debug))\n",
    "import requests\n",
    "requests.post('http://localhost:3000/api/versions', json=chunks4debug[:8] + chunks4debug[15:20])\n",
    "\n",
    "# chunk = chunks4debug[0]\n",
    "# print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 收集数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统一将数据格式化为 conflictMap\n",
    "```json\n",
    "{\n",
    "    \"path\": , // 文件相对路径\n",
    "    \"repo_url\": , // 仓库地址\n",
    "    \"file_a_content\": , // 文件 A 内容\n",
    "    \"file_b_content\": , // 文件 B 内容\n",
    "    \"file_o_content\": , // 文件 base 内容\n",
    "    \"file_r_content\": , // 文件 Resolved 内容\n",
    "    \"file_m_content\": , // 文件 Merged 内容\n",
    "    \"commitHash\": ,     // commit hash\n",
    "    \"conflict_chunks\": [\n",
    "        {\n",
    "            \"m_start\": , // merge 起始行\n",
    "            \"m_end\": , // merge 结束行\n",
    "            \"a_content\": , // A 内容\n",
    "            \"b_content\": , // B 内容\n",
    "            \"o_content\": , // base 内容\n",
    "            \"r_content\": , // resolved 内容\n",
    "            \"label\": , // conflict 类型\n",
    "            \"chunk_idx\": , // chunk 在文件中是第几个 chunk     // 有可能有的 chunk 没有 resolutioin\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = work_dir / \"data\" / \"congra_dataset\"\n",
    "output_dir = work_dir / \"data_collect_analysis\" / \"output\" / \"congra_dataset\"\n",
    "\n",
    "cc_cnt = 0\n",
    "illegal_cnt = 0\n",
    "\n",
    "class CONGRACollector(ConflictFileCollector):\n",
    "    '''\n",
    "    从 CONGRA 数据集中收集冲突文件\n",
    "    '''\n",
    "    def __init__(self, dataset_path):\n",
    "        super().__init__(dataset_path)\n",
    "    \n",
    "    def collect(self):\n",
    "        # 1. 获取所有冲突文件路径 如 data_dir / 'C_C++/git/conflict_files_0/regions' 下的所有文件名，比如 a.java.region\n",
    "        # 2. 第一行之后 (168, 192, 171, 195) 对应(origin_conflict_start, origin_conflict_end, resolved_start, resolved_end)，获得 n 个 tuple\n",
    "        # 3. 读取 C_C++/git/conflict_files_0/merged/a.java，逐行扫描得到冲突块，assert len(cc) == n\n",
    "        # 4. 读取 C_C++/git/conflict_files_0/resolved/a.java，获取(resolved_start, resolved_end)对应的内容\n",
    "        conflict_files_dirs = []\n",
    "        for root, dirs, files in os.walk(self.dataset_path):\n",
    "            for d in dirs:\n",
    "                if d.startswith('conflict_files_'):\n",
    "                    conflict_files_dirs.append(os.path.join(root, d))\n",
    "\n",
    "        print(len(conflict_files_dirs), '个冲突文件')\n",
    "\n",
    "        regions = []\n",
    "\n",
    "        # Step 1: 获取所有冲突文件路径\n",
    "        for conflict_dir in tqdm(conflict_files_dirs):\n",
    "            regions_dir = os.path.join(conflict_dir, 'regions')\n",
    "            if os.path.exists(regions_dir) and os.path.isdir(regions_dir):\n",
    "                for filename in os.listdir(regions_dir):\n",
    "                    regions.append(os.path.join(regions_dir, filename))\n",
    "\n",
    "        conflict_data = []\n",
    "\n",
    "        # Step 2: 解析冲突文件，获得 n 个 tuple\n",
    "        for region_file in regions:\n",
    "            with open(region_file, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                tuples = [tuple(map(int, line.strip()[1:-1].split(','))) for line in lines[1:]]\n",
    "                conflict_data.append((region_file, tuples))\n",
    "\n",
    "        print(len(conflict_data), '个冲突')\n",
    "        # debug\n",
    "        # conflict_data = conflict_data[:5]\n",
    "\n",
    "        for region_file, tuples in tqdm(conflict_data):\n",
    "            try:\n",
    "                base_name = os.path.basename(region_file).replace('.region', '')\n",
    "                merged_file_path = os.path.join(region_file.split('/regions/')[0], 'merged', base_name)\n",
    "                resolved_file_path = os.path.join(region_file.split('/regions/')[0], 'resolved', base_name)\n",
    "                with open(merged_file_path, 'r') as f:\n",
    "                    m_content = f.read()\n",
    "                with open(resolved_file_path, 'r') as f:\n",
    "                    r_content = f.read()\n",
    "                # 新建 ConflictFile 对象\n",
    "                conflict_file = ConflictFile(merged_file_path, str(merged_file_path), '', '', '', '', '', '')\n",
    "\n",
    "                # Step 3: 读取 merged 文件，逐行扫描得到冲突块\n",
    "                with open(merged_file_path, 'r') as merged_file:\n",
    "                    merged_lines = merged_file.readlines()\n",
    "                    # 逐行扫描\n",
    "                    for i, line in enumerate(merged_lines):\n",
    "                        if line.startswith('<<<<<<< a'):\n",
    "                            rec = i + 1\n",
    "                            m_start = i\n",
    "                        if line.startswith('||||||| base'):\n",
    "                            a_content: str = ''.join(merged_lines[rec:i])\n",
    "                            rec = i + 1\n",
    "                        if line.startswith('======='):\n",
    "                            o_content: str = ''.join(merged_lines[rec:i])\n",
    "                            rec = i + 1\n",
    "                        if line.startswith('>>>>>>> b'):\n",
    "                            b_content: str = ''.join(merged_lines[rec:i])\n",
    "                            m_end = i\n",
    "                            cc = ConflictChunk(m_start, m_end, a_content, b_content, o_content, '', None, len(conflict_file.conflict_chunks)) # label 和 r_content 暂时不填\n",
    "                            conflict_file.add_conflict_chunk(cc)\n",
    "                    assert len(conflict_file.conflict_chunks) == len(tuples)\n",
    "\n",
    "                # Step 4: 读取 resolved 文件，获取(resolved_start, resolved_end)对应的内容\n",
    "                with open(resolved_file_path, 'r') as resolved_file:\n",
    "                    resolved_lines = resolved_file.readlines()\n",
    "                    for i,(_, _, resolved_start, resolved_end) in enumerate(tuples):\n",
    "                        if resolved_end - 1 > len(resolved_lines) or resolved_start > resolved_end:\n",
    "                            print('resolved_end 超出 resolved 文件长度')\n",
    "                        conflict_file.conflict_chunks[i].r_content = ''.join(resolved_lines[resolved_start: resolved_end - 1])\n",
    "                        conflict_file.conflict_chunks[i].label = ConflictFileCollector.getLabel(conflict_file.conflict_chunks[i].a_content, \n",
    "                                                                                            conflict_file.conflict_chunks[i].b_content, \n",
    "                                                                                            conflict_file.conflict_chunks[i].o_content, \n",
    "                                                                                            conflict_file.conflict_chunks[i].r_content)\n",
    "                global cc_cnt\n",
    "                cc_cnt += len(conflict_file.conflict_chunks)\n",
    "                yield conflict_file\n",
    "            except Exception as e:\n",
    "                # print('Error:', e)\n",
    "                global illegal_cnt\n",
    "                illegal_cnt += 1\n",
    "                continue\n",
    "\n",
    "collector = CONGRACollector(data_dir)\n",
    "collector.collect_and_save(output_dir)\n",
    "print('illegal file: ', illegal_cnt)\n",
    "print('conflict chunk: ', cc_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/root/projects/gitMergeScenario/collect_output/output/conflictFiles\"\n",
    "output_dir = work_dir / \"data_collect_analysis\" / \"output\" / \"100+stars_4GB-_multidev_org_lang\"\n",
    "\n",
    "class GraphQLFilteredRepoCollector(ConflictFileCollector):\n",
    "    '''\n",
    "    100+ stars, non_fork, 10+devs, org, 4GB- repos on GitHub\n",
    "    '''\n",
    "    def __init__(self, dataset_path):\n",
    "        super().__init__(dataset_path)\n",
    "    \n",
    "    def collect(self):\n",
    "        # 1. 获取所有 json 文件名 /xxx/conflictFiles/hash/conflictFilesMetadata.json\n",
    "        # 3. 读取 json 文件，构造 ConflictFile 对象\n",
    "\n",
    "        metadata_jsonPaths = [path for path in self.getAllJsonsUnder(self.dataset_path)]\n",
    "        if len(metadata_jsonPaths) == 0:\n",
    "            raise FileNotFoundError(\"No metadata json files found in the dataset path\")\n",
    "\n",
    "        for jsonPath in tqdm(metadata_jsonPaths):\n",
    "            # 提取路径\n",
    "            basename = os.path.basename(jsonPath)\n",
    "            if basename != 'conflictFilesMetadata.json':\n",
    "                raise ValueError(\"conflictFilesMetadata.json file name error\")\n",
    "            dirname = os.path.dirname(jsonPath)\n",
    "\n",
    "            ret = []\n",
    "            # jsonData\n",
    "            with open(jsonPath, 'r') as f:      # 好多数据都没收集\n",
    "                try:\n",
    "                    metadata_list = json.load(f)\n",
    "                    for metadata in metadata_list:\n",
    "                        repo_url = None                 # 还真没记录 repo_url 或者 author/repoName，只记录 repoName 了\n",
    "                        path = metadata['filePath']\n",
    "                        suffix = path.split('.')[-1]\n",
    "                        conflictChunks = metadata['conflictChunks']\n",
    "                        commit_hash = metadata['resolvedCommitHash']\n",
    "\n",
    "                        a_content = '\\n'.join(metadata['oursContent'])          # 不需要在最后 + '\\n'，收集数据集是用的是 String.split('\\n', -1) -1 代表尽量分割，所以 join 后不需要再加换行符\n",
    "                        b_content = '\\n'.join(metadata['theirsContent'])\n",
    "                        base_content = '\\n'.join(metadata['baseContent'])\n",
    "                        merged_content = '\\n'.join(metadata['mergedContent'])\n",
    "                        r_content = '\\n'.join(metadata['resolvedContent'])\n",
    "                \n",
    "                    # 构造 ConflictFile 对象\n",
    "                    conflict_file = ConflictFile(path, repo_url, a_content, b_content, base_content, r_content, merged_content, commit_hash)\n",
    "\n",
    "                    # 过滤 .min.xx .bundle.xx\n",
    "                    if '.min.' in path or '.bundle.' in path:\n",
    "                        continue\n",
    "                    # 过滤内容只有很长的一行的文件，例如某种形式的 .min.js\n",
    "                    for content in [metadata['oursContent'], metadata['theirsContent'], metadata['baseContent'], metadata['resolvedContent']]:\n",
    "                        if len(content) == 1 and len(content[0]) > 2000:\n",
    "                            continue\n",
    "\n",
    "\n",
    "                    for chunk in conflictChunks:\n",
    "                        if 'resolution' not in chunk or chunk['resolution'] == None:                          # gitMergeScenario 中 DeepMergeAligner 没有找到 resolution\n",
    "                            continue\n",
    "                        # m_start, m_end 和 chunk_idx 不太好拿，先忽略\n",
    "                        # 最后得加 \\n，因为 DeepMergeAligner 获取 resolution 是从代码行数组中提取出来的行，所以要加上换行符\n",
    "                        cc = ConflictChunk(\n",
    "                                chunk['startLine'], \n",
    "                                chunk['endLine'], \n",
    "                                self.list2str(chunk['ours']), \n",
    "                                self.list2str(chunk['theirs']), \n",
    "                                self.list2str(chunk['base']), \n",
    "                                self.list2str(chunk['resolution']), \n",
    "                                None, None)\n",
    "                        cc.label = self.getLabel(cc.a_content, cc.b_content, cc.o_content, cc.r_content)\n",
    "                        conflict_file.add_conflict_chunk(cc)\n",
    "                    ret.append(conflict_file)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {jsonPath}: {e} (type: {type(e).__name__})\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()  # 打印完整堆栈信息\n",
    "            for conflict_file in ret:\n",
    "                yield conflict_file\n",
    "\n",
    "collector = GraphQLFilteredRepoCollector(data_dir)\n",
    "collector.collect_and_save(output_dir)\n",
    "\n",
    "# todo: 为什么有很多 chunk 没有 mergedContent\n",
    "\n",
    "# 这里收集的 file merged content 的冲突块范围是由 jgit 的 formatter 生成的，有一个问题是会排除 AB 中的公共行\n",
    "# 另有一个脚本可以在这个基础上生成冲突块，但是不排除 AB 中的公共行（采用 git merge-file）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在已经输出 JSON 的情况下，临时处理一下 .min.js 和 .bundle.js 文件\n",
    "# tmp content，某次收集后的补救措施\n",
    "\n",
    "data_dir = Path(\"/Volumes/urine_bag/100+stars_4GB-_multidev_org_lang\")\n",
    "# output_dir = work_dir / \"data_collect_analysis\" / \"output\" / \"100+stars\"\n",
    "output_dir = Path(\"/Volumes/urine_bag/recollect_without_min_bundle_without_file_content\")\n",
    "\n",
    "# ensure output_dir exists\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def filter_cf(cf: dict) -> bool:\n",
    "    if '.min.' in cf['path'] or '.bundle.' in cf['path']:\n",
    "        return False\n",
    "    for content in [cf['file_a_content'], cf['file_b_content'], cf['file_o_content'], cf['file_r_content']]:\n",
    "        if len(content) > 2000 and content.count('\\n') <= 1:\n",
    "            return False\n",
    "    return True\n",
    "# 1. 读取所有 JSON 文件\n",
    "# 2. 读取所有 JSON 文件中的 ConflictFile\n",
    "# 3. 过滤 .min.js 和 .bundle.js 文件，过滤超长一行\n",
    "# 4. 保存到 output_dir\n",
    "jsons = list(data_dir.glob('*.json'))\n",
    "for json_file in tqdm(jsons):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 只保留几个字段\n",
    "    filtered_data = [{\n",
    "        \"path\": cf['path'],\n",
    "        \"repo_url\": cf['repo_url'],\n",
    "        \"commit_hash\": cf['commit_hash'],\n",
    "        \"conflict_chunks\": cf['conflict_chunks'],\n",
    "    } for cf in data if filter_cf(cf)]\n",
    "    with open(output_dir / json_file.name, 'w') as f:\n",
    "        json.dump(filtered_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = work_dir / \"data\" / \"2000repos\"\n",
    "output_dir = work_dir / \"data_collect_analysis\" / \"output\" / \"2000repos\"\n",
    "\n",
    "class MergeNatureRepoCollector(ConflictFileCollector):\n",
    "    '''\n",
    "    2000 repos 数据集转化为 conflictMap\n",
    "    '''\n",
    "    def __init__(self, dataset_path):\n",
    "        super().__init__(dataset_path)\n",
    "\n",
    "    def collect(self):\n",
    "        # 1. 获取所有 json 文件名 /xxx/repo_name/hash/relativePath/filename/metadata.json\n",
    "        # 2. 提取对应目录下的 ours.xxx theirs.xxx base.xxx conflict.xxx resolve.xxx\n",
    "        # 3. 读取 metadata.json, 获取 repo_url, path 以及 conflict chunks\n",
    "\n",
    "        metadata_jsonPaths = [path for path in self.getAllJsonsUnder(self.dataset_path)]\n",
    "        if len(metadata_jsonPaths) == 0:\n",
    "            raise FileNotFoundError(\"No metadata json files found in the dataset path\")\n",
    "        for jsonPath in tqdm(metadata_jsonPaths):\n",
    "            # 提取路径\n",
    "            basename = os.path.basename(jsonPath)\n",
    "            if basename != 'metadata.json':\n",
    "                raise ValueError(\"metadata.json file name error\")\n",
    " \n",
    "            # jsonData\n",
    "            with open(jsonPath, 'r') as f:      # 好多数据都没收集\n",
    "                metadata = json.load(f)\n",
    "                repo_url = None\n",
    "                path = metadata['path']\n",
    "                suffix = metadata['filetype']\n",
    "                conflict_chunks = metadata['conflicting_chunks']\n",
    "                commit_hash = metadata['commitID']\n",
    "            dirname = os.path.dirname(jsonPath)\n",
    "            # 读取 a, b, base, merged, resolved\n",
    "            a_path = os.path.join(dirname, 'ours' + suffix)\n",
    "            b_path = os.path.join(dirname, 'theirs' + suffix)\n",
    "            base_path = os.path.join(dirname, 'base' + suffix)\n",
    "            merged_path = os.path.join(dirname, 'conflict' + suffix)\n",
    "            resolved_path = os.path.join(dirname, 'resolve' + suffix)\n",
    "\n",
    "            # 读取文件内容\n",
    "            try:\n",
    "                with open(a_path, 'r') as f:\n",
    "                    a_content = f.read()\n",
    "                with open(b_path, 'r') as f:\n",
    "                    b_content = f.read()\n",
    "                with open (base_path, 'r') as f:\n",
    "                    base_content = f.read()\n",
    "                with open (merged_path, 'r') as f:\n",
    "                    merged_content = f.read()\n",
    "                with open (resolved_path, 'r') as f:\n",
    "                    r_content = f.read()\n",
    "            except Exception as e:\n",
    "                # 有的文件不存在，直接跳过\n",
    "                # print(jsonPath)\n",
    "                # print(e)\n",
    "                continue\n",
    "            \n",
    "            # 构造 ConflictFile 对象\n",
    "            conflict_file = ConflictFile(path, repo_url, a_content, b_content, base_content, r_content, merged_content, commit_hash)\n",
    "            for chunk in conflict_chunks:\n",
    "                if 'resolve' not in chunk or chunk['resolve'] == None:\n",
    "                    continue\n",
    "                # m_start, m_end 和 chunk_idx 不太好拿，先忽略\n",
    "                cc = ConflictChunk(-1, -1, chunk['a_contents'], chunk['b_contents'], \n",
    "                                    chunk['base_contents'], chunk['resolve'], None, None)\n",
    "                cc.label = self.getLabel(cc.a_content, cc.b_content, cc.o_content, cc.r_content)\n",
    "                conflict_file.add_conflict_chunk(cc)\n",
    "            yield conflict_file\n",
    "\n",
    "\n",
    "collector = MergeNatureRepoCollector(data_dir)\n",
    "collector.collect_and_save(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = work_dir / \"data\" / \"top50\"\n",
    "output_dir = work_dir / \"data_collect_analysis\" / \"output\" / \"top50\"\n",
    "\n",
    "class MergeNatureRepoTop50Collector(ConflictFileCollector):\n",
    "    '''\n",
    "    top50/2000 repos 数据集转化为 conflictMap\n",
    "    '''\n",
    "    def __init__(self, dataset_path):\n",
    "        super().__init__(dataset_path)\n",
    "    \n",
    "    def collect(self):\n",
    "        # 1. 获取所有 json 文件名 /.../repo_name/12345_a.java\n",
    "        #    提取最后的 12345\n",
    "        # 2. 获取对应的 12345_a.xxx, 12345_b.xxx, 12345_base.xxx, 12345_merged.xxx, 12345_resolved.xxx\n",
    "        # 3. 读取 metadata.json, 获取 repo_url, path 以及 conflict chunks\n",
    "\n",
    "        metadata_jsonPaths = [path for path in self.getAllJsonsUnder(self.dataset_path)]\n",
    "        if len(metadata_jsonPaths) == 0:\n",
    "            raise FileNotFoundError(\"No metadata json files found in the dataset path\")\n",
    "        for jsonPath in tqdm(metadata_jsonPaths):\n",
    "            # 提取路径\n",
    "            basename = os.path.basename(jsonPath)\n",
    "            dirname = os.path.dirname(jsonPath)\n",
    "\n",
    "            # jsonData\n",
    "            with open(jsonPath, 'r') as f:      # 好多数据都没收集\n",
    "                metadata = json.load(f)\n",
    "                repo_url = None\n",
    "                path = None\n",
    "                suffix = metadata['filetype']\n",
    "                conflict_chunks = metadata['conflicting_chunks']\n",
    "                commit_hash = None\n",
    "            \n",
    "            # 读取 a, b, base, merged, resolved\n",
    "            a_path = os.path.join(dirname, basename.replace('_metadata.json', '_a' + suffix))\n",
    "            b_path = os.path.join(dirname, basename.replace('_metadata.json', '_b' + suffix))\n",
    "            base_path = os.path.join(dirname, basename.replace('_metadata.json', '_base' + suffix))\n",
    "            merged_path = os.path.join(dirname, basename.replace('_metadata.json', '_merged' + suffix))\n",
    "            resolved_path = os.path.join(dirname, basename.replace('_metadata.json', '_resolved' + suffix))\n",
    "\n",
    "            # 读取文件内容\n",
    "            try:\n",
    "                with open(a_path, 'r') as f:\n",
    "                    a_content = f.read()\n",
    "                with open(b_path, 'r') as f:\n",
    "                    b_content = f.read()\n",
    "                with open (base_path, 'r') as f:\n",
    "                    base_content = f.read()\n",
    "                with open (merged_path, 'r') as f:\n",
    "                    merged_content = f.read()\n",
    "                with open(resolved_path, 'r') as f:\n",
    "                    resolved_content = f.read()\n",
    "            except Exception as e:\n",
    "                print(jsonPath)\n",
    "                print(e)\n",
    "                continue\n",
    "            \n",
    "            # 构造 ConflictFile 对象\n",
    "            conflict_file = ConflictFile(path, repo_url, a_content, b_content, base_content, resolved_content, merged_content, commit_hash)\n",
    "            for chunk in conflict_chunks:\n",
    "                if 'resolve' not in chunk or chunk['resolve'] == None:\n",
    "                    continue\n",
    "                # m_start, m_end 和 chunk_idx 不太好拿，先忽略\n",
    "                cc = ConflictChunk(-1, -1, chunk['a_contents'], chunk['b_contents'], \n",
    "                                    chunk['base_contents'], chunk['resolve'], None, None)\n",
    "                cc.label = self.getLabel(cc.a_content, cc.b_content, cc.o_content, cc.r_content)\n",
    "                conflict_file.add_conflict_chunk(cc)\n",
    "            yield conflict_file\n",
    "\n",
    "collector = MergeNatureRepoTop50Collector(data_dir)\n",
    "collector.collect_and_save(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = work_dir / \"data\" / \"mergebert_data\" / \"automated-analysis-data\" / \"TypeScript\"\n",
    "# output_dir = work_dir / \"data_collect_analysis\" / \"output\" / \"mergebert_ts\"\n",
    "\n",
    "data_dir = work_dir / \"data\" / \"mergebert_data\" / \"automated-analysis-data\"\n",
    "output_dir = work_dir / \"data_collect_analysis\" / \"output\" / \"mergebert_all_lang\"\n",
    "\n",
    "class MergeBERTConflictFileCollector(ConflictFileCollector):\n",
    "    '''\n",
    "    MergeBERT 数据集转化为 conflictMap\n",
    "    '''\n",
    "    def __init__(self, dataset_path):\n",
    "        super().__init__(dataset_path)\n",
    "    \n",
    "    def collect(self):\n",
    "        # 1. 获取所有 json 文件名，如 /Users/foril/projects/conflict_resolve/my_work/dataset_collect_analysis_script/data/mergebert_data/automated-analysis-data/TypeScript/55743_metadata.json,\n",
    "        #    提取最后的 12345\n",
    "        # 2. 获取对应的 12345_a.xxx, 12345_b.xxx, 12345_base.xxx, 12345_merged.xxx, 12345_resolved.xxx\n",
    "        # 3. 读取 metadata.json, 获取 repo_url, path 以及 conflict chunks\n",
    "        chunk_cnt = 0\n",
    "        chunk_no_r_cnt = 0\n",
    "\n",
    "        metadata_jsonPaths = [path for path in self.getAllJsonsUnder(self.dataset_path)]\n",
    "        if len(metadata_jsonPaths) == 0:\n",
    "            raise FileNotFoundError(\"No metadata json files found in the dataset path\")\n",
    "        for jsonPath in tqdm(metadata_jsonPaths):\n",
    "            # 提取路径\n",
    "            basename = os.path.basename(jsonPath)\n",
    "            dirname = os.path.dirname(jsonPath)\n",
    "\n",
    "            # jsonData\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "                repo_url = metadata['repo']\n",
    "                path = metadata['fname']\n",
    "                suffix = path.split('.')[-1]\n",
    "                conflict_chunks = metadata['conflicting_chunks']\n",
    "                commit_hash = metadata['commitHash']\n",
    "            \n",
    "            # 读取 a, b, base, merged, resolved\n",
    "            a_path = os.path.join(dirname, basename.replace('_metadata.json', '_a.' + suffix))\n",
    "            b_path = os.path.join(dirname, basename.replace('_metadata.json', '_b.' + suffix))\n",
    "            base_path = os.path.join(dirname, basename.replace('_metadata.json', '_base.' + suffix))\n",
    "            merged_path = os.path.join(dirname, basename.replace('_metadata.json', '_merged.' + suffix))\n",
    "            resolved_path = os.path.join(dirname, basename.replace('_metadata.json', '_resolved.' + suffix))\n",
    "\n",
    "            # 读取文件内容\n",
    "            with open(a_path, 'r') as f:\n",
    "                a_content = f.read()\n",
    "            with open(b_path, 'r') as f:\n",
    "                b_content = f.read()\n",
    "            with open (base_path, 'r') as f:\n",
    "                base_content = f.read()\n",
    "            with open (merged_path, 'r') as f:\n",
    "                merged_content = f.read()\n",
    "            with open(resolved_path, 'r') as f:\n",
    "                resolved_content = f.read()\n",
    "            \n",
    "            # 构造 ConflictFile 对象\n",
    "            conflict_file = ConflictFile(path, repo_url, a_content, b_content, base_content, resolved_content, merged_content, commit_hash)\n",
    "            for chunk in conflict_chunks:\n",
    "                chunk_cnt += 1\n",
    "                if chunk['res_region'] == None:\n",
    "                    chunk_no_r_cnt += 1\n",
    "                    continue\n",
    "                # m_start, m_end 和 chunk_idx 不太好拿，对 MergeBERT 数据集好像也不是很重要，先忽略吧\n",
    "                cc = ConflictChunk(-1, -1, chunk['a_contents'], chunk['b_contents'], \n",
    "                                    chunk['base_contents'], chunk['res_region'], None, None)\n",
    "                cc.mergebert_label = chunk.get('label', None) # type: ignore\n",
    "                    # 'A',\n",
    "                    #  'AB',\n",
    "                    #  'B',\n",
    "                    #  'BA',\n",
    "                    #  'BASE',\n",
    "                    #  None,\n",
    "                    #  'OTHER',\n",
    "                    #  'REM-BASE-A',\n",
    "                    #  'REM-BASE-AB',\n",
    "                    #  'REM-BASE-B',\n",
    "                    #  'REM-BASE-BA',\n",
    "                    #  'RES_EMPTY',\n",
    "                    #  'RES_FILE_EMPTY'\n",
    "                \n",
    "                cc.label = self.getLabel(cc.a_content, cc.b_content, cc.o_content, cc.r_content)\n",
    "\n",
    "                conflict_file.add_conflict_chunk(cc)\n",
    "            yield conflict_file\n",
    "        print(f\"Total chunk count: {chunk_cnt}, chunk without r: {chunk_no_r_cnt}\")\n",
    "\n",
    "collector = MergeBERTConflictFileCollector(data_dir)\n",
    "collector.collect_and_save(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 分析冲突块的类型分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件夹下所有 json 文件，统计 ConflictFile 下的 ConflictChunk 的 label 分布\n",
    "dir2analyze = work_dir / \"data_collect_analysis\" / \"output\" / \"100+stars_sample\"\n",
    "dir2analyze = work_dir / \"data_collect_analysis\" / \"output\" / \"congra_dataset\"\n",
    "# dir2analyze = work_dir / \"data_collect_analysis\" / \"output\" / \"100+_recollect\"\n",
    "# dir2analyze = work_dir / \"data_collect_analysis\" / \"output\" / \"mergebert_ts\"\n",
    "\n",
    "# 输入存放 ConflictFiles 的目录，输出 类型分布 map，同时绘制饼图\n",
    "def analyze_label_distribution(dir2analyze):\n",
    "    label_cnt = defaultdict(int)\n",
    "    # 获取所有 json 文件名\n",
    "    jsonPaths = [path for path in ConflictFileCollector.getAllJsonsUnder(dir2analyze)]\n",
    "    if len(jsonPaths) == 0:\n",
    "        raise FileNotFoundError(\"No metadata json files found in the dataset path\")\n",
    "    for jsonPath in tqdm(jsonPaths, position=0, leave=True, dynamic_ncols=True):\n",
    "        # jsonData\n",
    "        with open(jsonPath, 'r') as f:\n",
    "            try:\n",
    "                for x in tqdm(json.load(f), position=1, leave=False, dynamic_ncols=True):\n",
    "                    for chunk in x['conflict_chunks']:\n",
    "                        ### tmp \n",
    "                        # 因为 bug，导致有的没有找到 resolution 的 chunk 也加入了，这里忽略\n",
    "                        if 'label' not in chunk:\n",
    "                            continue\n",
    "                        ### tmp\n",
    "                        label_cnt[chunk['label']] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {jsonPath}: {e} (type: {type(e).__name__})\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "    import plotly.graph_objects as go\n",
    "    # 创建饼图\n",
    "    labels = list(label_cnt.keys())\n",
    "    values = list(label_cnt.values())\n",
    "    fig = go.Figure(data=[go.Pie(labels=labels, values=values)])\n",
    "    # 设置布局\n",
    "    fig.update_layout(title_text=\"各类型冲突占比\", width=600, height=400)\n",
    "    # 显示图形\n",
    "    fig.show()\n",
    "    from pprint import pprint\n",
    "    pprint(label_cnt)\n",
    "    return label_cnt\n",
    "\n",
    "\n",
    "# 从文件中读取\n",
    "label_cnt = analyze_label_distribution(dir2analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析一个文件中的冲突块数量分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件夹下所有 json 文件，统计 ConflictFile 下的 ConflictChunk 的数量分布\n",
    "dir2analyze = work_dir / \"data_collect_analysis\" / \"output\" / \"100+stars_sample\"\n",
    "# dir2analyze = work_dir / \"data_collect_analysis\" / \"output\" / \"congra_dataset\"\n",
    "# dir2analyze = work_dir / \"data_collect_analysis\" / \"output\" / \"100+_recollect\"\n",
    "# dir2analyze = work_dir / \"data_collect_analysis\" / \"output\" / \"mergebert_ts\"\n",
    "\n",
    "# 输入存放 ConflictFiles 的目录，统计每个 ConflictFile 下的 ConflictChunk 数量，绘制直方图\n",
    "def analyze_chunk_num_distribution(dir2analyze):\n",
    "    num_cnt = defaultdict(int)\n",
    "    # 获取所有 json 文件名\n",
    "    jsonPaths = [path for path in ConflictFileCollector.getAllJsonsUnder(dir2analyze)]\n",
    "    if len(jsonPaths) == 0:\n",
    "        raise FileNotFoundError(\"No metadata json files found in the dataset path\")\n",
    "    for jsonPath in tqdm(jsonPaths, position=0, leave=True, dynamic_ncols=True):\n",
    "        # jsonData\n",
    "        with open(jsonPath, 'r') as f:\n",
    "            try:\n",
    "                for x in tqdm(json.load(f), position=1, leave=False, dynamic_ncols=True):\n",
    "                    num_cnt[len(x['conflict_chunks'])] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {jsonPath}: {e} (type: {type(e).__name__})\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "    import plotly.graph_objects as go\n",
    "    x = list(num_cnt.keys())\n",
    "    y = list(num_cnt.values())\n",
    "    # 计算百分比\n",
    "    total = sum(y)\n",
    "    percentages = [f\"{(value / total) * 100:.2f}%\" for value in y]\n",
    "    # 创建直方图并添加百分比标注\n",
    "    fig = go.Figure(data=[go.Bar(x=x, y=y, text=percentages, textposition='outside')])\n",
    "    # 设置布局\n",
    "    fig.update_layout(title_text=\"冲突块数量分布\", width=600, height=400)\n",
    "    # 显示图形\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# 从文件中读取\n",
    "analyze_chunk_num_distribution(dir2analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取指定语言数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件夹下所有 json 文件，提取指定语言的文件中的合并冲突块\n",
    "# dir2analyze = work_dir / \"data_collect_analysis\" / \"output\" / \"mergebert_all_lang\"\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "dir2analyze = \"/Volumes/Q1571825323/recollect_without_min_bundle_without_file_content\"\n",
    "\n",
    "codebert_conflict_files_output_dir = work_dir / \"data_collect_analysis\" / \"output\" / \"codebert_conflict_files\"\n",
    "zero_shot_ext_conflict_files_output_dir = work_dir / \"data_collect_analysis\" / \"output\" / \"zero_shot_conflict_files\"\n",
    "\n",
    "# 经过分析\n",
    "codebert_lang_ext = {'.go', '.java', '.rb', '.js', '.py', '.php'}\n",
    "zero_shot_ext = {'.h', '.mm', '.cs', '.swift', '.rs', '.c', '.ts', '.m', '.hpp', '.cpp'}\n",
    "\n",
    "codebert_conflict_files = []\n",
    "zero_shot_ext_conflict_files = []\n",
    "# 获取所有 json 文件名\n",
    "jsonPaths = [path for path in ConflictFileCollector.getAllJsonsUnder(dir2analyze)]\n",
    "if len(jsonPaths) == 0:\n",
    "    raise FileNotFoundError(\"No metadata json files found in the dataset path\")\n",
    "for jsonPath in tqdm(jsonPaths, position=0, leave=True, dynamic_ncols=True):\n",
    "    # jsonData\n",
    "    with open(jsonPath, 'r') as f:\n",
    "        try:\n",
    "            for x in tqdm(json.load(f), position=1, leave=False, dynamic_ncols=True):\n",
    "                ext = os.path.splitext(x['path'])[1]\n",
    "                if ext in codebert_lang_ext:\n",
    "                    codebert_conflict_files.append(x)\n",
    "                elif ext in zero_shot_ext:\n",
    "                    zero_shot_ext_conflict_files.append(x)\n",
    "                else:\n",
    "                    assert False\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {jsonPath}: {e} (type: {type(e).__name__})\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# 分文件保存\n",
    "os.makedirs(codebert_conflict_files_output_dir, exist_ok=True)\n",
    "for i in tqdm(range(ceil(len(codebert_conflict_files) / 10000)), position=0, leave=True, dynamic_ncols=True):\n",
    "    with open(codebert_conflict_files_output_dir / f\"{i}.json\", 'w') as f:\n",
    "        json.dump(codebert_conflict_files[i * 10000: min((i + 1) * 10000, len(codebert_conflict_files))], f)\n",
    "\n",
    "os.makedirs(zero_shot_ext_conflict_files_output_dir, exist_ok=True)\n",
    "for i in tqdm(range(ceil(len(zero_shot_ext_conflict_files) / 10000)), position=0, leave=True, dynamic_ncols=True):\n",
    "    with open(zero_shot_ext_conflict_files_output_dir / f\"{i}.json\", 'w') as f:\n",
    "        json.dump(zero_shot_ext_conflict_files[i * 10000: min((i + 1) * 10000, len(zero_shot_ext_conflict_files))], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回溯分析解决上界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = []\n",
    "# dirs.append(work_dir / \"data_collect_analysis\" / \"output\" / \"mergebert_ts\")\n",
    "# dirs.append(work_dir / \"data_collect_analysis\" / \"output\" / \"mergebert_all_lang\")\n",
    "# dirs.append(work_dir / \"data_collect_analysis\" / \"output\" / \"100+stars_4GB-_multidev_org_lang\")\n",
    "dirs.append(work_dir / \"data_collect_analysis\" / \"output\" / \"100+stars_sample\")\n",
    "# dirs.append(work_dir / \"data_collect_analysis\" / \"output\" / \"2000repos\")\n",
    "# dirs.append(work_dir / \"data_collect_analysis\" / \"output\" / \"top50\")\n",
    "\n",
    "\n",
    "\n",
    "from util.edit_script import compute, SequenceDiff\n",
    "\n",
    "class EditScriptLabel:\n",
    "    def __init__(self, sd: SequenceDiff, _from: str, accept: bool):\n",
    "        self.edit_script = sd\n",
    "        self._from = _from\n",
    "        self.accept = accept\n",
    "\n",
    "def analyze_edit_script(dir2analyze):\n",
    "    dataset_name = os.path.basename(dir2analyze)\n",
    "    print(f'在 {dataset_name} 下统计')\n",
    "    accept_mark_cnt = defaultdict(int)\n",
    "    es_cnt = defaultdict(int)\n",
    "    # cc_with_es_intersects = 0\n",
    "    resolvable_cc_cnt = 0\n",
    "    non_resolvable_cc_cnt = 0\n",
    "    too_many_lines_cnt = 0\n",
    "    too_many_es_cnt = 0\n",
    "    label_cnt = defaultdict(int)\n",
    "    label_resolvable_cnt = defaultdict(int)\n",
    "\n",
    "    def cc_check(chunk: ConflictChunk) -> None:\n",
    "        '''\n",
    "        统计可以用编辑脚本解决的冲突，统计接受和拒绝的数量\n",
    "        统计编辑脚本的数量，如果太多则跳\n",
    "        最后比较时我希望转化成 token\n",
    "        生成编辑脚本时，去除空行影响，缩进。。。去掉？\n",
    "        '''\n",
    "        nonlocal accept_mark_cnt\n",
    "        nonlocal es_cnt\n",
    "        nonlocal resolvable_cc_cnt\n",
    "        nonlocal non_resolvable_cc_cnt\n",
    "        nonlocal too_many_lines_cnt\n",
    "        nonlocal too_many_es_cnt\n",
    "        nonlocal label_resolvable_cnt\n",
    "\n",
    "        def es_gen_str2list(content: str) -> List[str]:\n",
    "            '''\n",
    "            生成编辑脚本时的处理\n",
    "            '''\n",
    "            return [line.strip() for line in content.split('\\n') if line.strip() != '']\n",
    "            \n",
    "        a_contents = es_gen_str2list(chunk.a_content)\n",
    "        b_contents = es_gen_str2list(chunk.b_content)\n",
    "        o_contents = es_gen_str2list(chunk.o_content)\n",
    "        r_contents = es_gen_str2list(chunk.r_content)\n",
    "\n",
    "        def compareInToken(a_ls: List[str], b_ls: List[str]) -> bool:\n",
    "            '''\n",
    "            最后比较的预处理，忽略空白符的影响\n",
    "            '''\n",
    "            def toUnifiedStr(ls: List[str]) -> str:\n",
    "                return '' if ls == [] or ls == [''] else re.sub(r'\\s+', ' ', '\\n'.join(ls).strip() + '\\n')\n",
    "            a_processed = toUnifiedStr(a_ls)\n",
    "            b_processed = toUnifiedStr(b_ls)\n",
    "            # print(a_processed)\n",
    "            # print(b_processed)\n",
    "            # print(a_processed == b_processed)\n",
    "            # print('-' * 20)\n",
    "            return a_processed == b_processed\n",
    "\n",
    "        def bt(generated, i, last_end, all_edit_scripts: List[EditScriptLabel]) -> bool:\n",
    "            '''\n",
    "            回溯法生成所有可能的解决方案，如果和 resolution 相同则加入结果集\n",
    "            '''\n",
    "            # exit\n",
    "            if i == len(all_edit_scripts):\n",
    "                whole_generated = generated + o_contents[last_end:]\n",
    "                # 过滤 whole_generated 和 resolution 中的空行\n",
    "                if compareInToken(whole_generated, r_contents):\n",
    "                    # 可以使用组合 ES 的方式解决的冲突\n",
    "                    return True\n",
    "                return False\n",
    "\n",
    "            # 不接受这个脚本\n",
    "            all_edit_scripts[i].accept = False\n",
    "            if bt(generated, i + 1, last_end, all_edit_scripts):\n",
    "                return True\n",
    "\n",
    "            # 如果当前脚本的起始位置比 last_end 还小，说明这个脚本和上一个脚本有冲突\n",
    "            # 不能接受这个脚本，直接跳过\n",
    "            if all_edit_scripts[i].edit_script.seq1Range.start < last_end:\n",
    "                return False     # 因为是小于号，所以可以解决伪冲突\n",
    "\n",
    "            # 接受这个脚本\n",
    "            start = all_edit_scripts[i].edit_script.seq2Range.start\n",
    "            end = all_edit_scripts[i].edit_script.seq2Range.end\n",
    "            if all_edit_scripts[i]._from == 'ours':\n",
    "                curr_content = a_contents[start:end]\n",
    "            else:\n",
    "                curr_content = b_contents[start:end]\n",
    "            all_edit_scripts[i].accept = True\n",
    "            if bt(generated\n",
    "                    + o_contents[last_end:all_edit_scripts[i].edit_script.seq1Range.start]\n",
    "                    + curr_content,\n",
    "                    i + 1,\n",
    "                    all_edit_scripts[i].edit_script.seq1Range.end,\n",
    "                    all_edit_scripts\n",
    "                ):\n",
    "                return True\n",
    "\n",
    "\n",
    "            # 有下一个脚本，且两者对应 base 的位置相同\n",
    "            if (\n",
    "                i + 1 < len(all_edit_scripts) and\n",
    "                all_edit_scripts[i].edit_script.seq1Range == all_edit_scripts[i + 1].edit_script.seq1Range\n",
    "            ):\n",
    "                start = all_edit_scripts[i + 1].edit_script.seq2Range.start\n",
    "                end = all_edit_scripts[i + 1].edit_script.seq2Range.end\n",
    "                if all_edit_scripts[i + 1]._from == 'ours':\n",
    "                    next_content = a_contents[start:end]\n",
    "                else:\n",
    "                    next_content = b_contents[start:end]\n",
    "\n",
    "                # base 长度为 0 的情况，只需要加入另一种 concat（seq1Range 的长度为 0，代表双方在同一位置的插入）\n",
    "                all_edit_scripts[i + 1].accept = True\n",
    "                if bt(generated\n",
    "                        + o_contents[last_end:all_edit_scripts[i].edit_script.seq1Range.start]\n",
    "                        + next_content\n",
    "                        + curr_content,\n",
    "                    i + 2,\n",
    "                    all_edit_scripts[i].edit_script.seq1Range.end,\n",
    "                    all_edit_scripts\n",
    "                ):\n",
    "                    return True\n",
    "                # base 长度不为 0 的情况，需要考虑两种 concat\n",
    "                if len(all_edit_scripts[i].edit_script.seq1Range) > 0: \n",
    "                    all_edit_scripts[i + 1].accept = True\n",
    "                    if bt(generated\n",
    "                            + o_contents[last_end:all_edit_scripts[i].edit_script.seq1Range.start]\n",
    "                            + curr_content\n",
    "                            + next_content,\n",
    "                            i + 2,\n",
    "                            all_edit_scripts[i].edit_script.seq1Range.end,\n",
    "                            all_edit_scripts\n",
    "                    ):\n",
    "                        return True\n",
    "\n",
    "\n",
    "        # 开始收集数据集\n",
    "            \n",
    "        # 如果行数过大，直接跳过\n",
    "        if any([len(content) > 200 for content in [a_contents, b_contents, o_contents, r_contents]]):\n",
    "            too_many_lines_cnt += 1\n",
    "            return\n",
    "        \n",
    "        \n",
    "        from_ours = compute(o_contents, a_contents)\n",
    "        from_theirs = compute(o_contents, b_contents)\n",
    "        # 加入 _from 标记\n",
    "        from_ours = [EditScriptLabel(sd, 'ours', False) for sd in from_ours]\n",
    "        from_theirs = [EditScriptLabel(sd, 'theirs', False) for sd in from_theirs]\n",
    "        all_edit_scripts = from_ours + from_theirs\n",
    "        es_cnt[len(all_edit_scripts)] += 1              # 统计编辑脚本数量，这里已经过滤了行数过大\n",
    "        \n",
    "        \n",
    "        # 限制脚本数量，避免计算量过大\n",
    "        if len(all_edit_scripts) > 10:\n",
    "            too_many_es_cnt += 1\n",
    "            return\n",
    "        \n",
    "\n",
    "        all_edit_scripts.sort(key=lambda editScriptLabel: editScriptLabel.edit_script.seq1Range)\n",
    "\n",
    "        kind = chunk.label\n",
    "        label_cnt[kind] += 1\n",
    "        # 如果是 newline 的冲突，直接跳过\n",
    "        if kind == 'newline':\n",
    "            non_resolvable_cc_cnt += 1\n",
    "            return\n",
    "        if kind == 'same modification, formatting maybe different':\n",
    "            resolvable_cc_cnt += 1\n",
    "            label_resolvable_cnt[kind] += 1\n",
    "            return\n",
    "\n",
    "        if bt([], 0, 0, all_edit_scripts):  # 这个冲突能解决\n",
    "            resolvable_cc_cnt += 1\n",
    "            label_resolvable_cnt[kind] += 1\n",
    "            # 统计 accept_mark\n",
    "            for i, es in enumerate(all_edit_scripts):\n",
    "                accept_mark_cnt[es.accept] += 1\n",
    "        else:\n",
    "            non_resolvable_cc_cnt += 1\n",
    "\n",
    "\n",
    "\n",
    "    # 开始统计数据集结果\n",
    "    jsonPaths = [path for path in ConflictFileCollector.getAllJsonsUnder(dir2analyze)]\n",
    "    if len(jsonPaths) == 0:\n",
    "        raise FileNotFoundError(\"No metadata json files found in the dataset path\")\n",
    "    for jsonPath in tqdm(jsonPaths, desc=\"Processing files\", position=0, leave=True, dynamic_ncols=True):\n",
    "        # jsonData\n",
    "        try:\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                cfs = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {jsonPath}: {e} (type: {type(e).__name__})\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        for cf in tqdm(cfs, desc=f\"Process items\", position=1, leave=False, dynamic_ncols=True):\n",
    "            for cc in cf['conflict_chunks']:\n",
    "                # all_cc_cnt += 1\n",
    "                # label_cnt[cc['label']] += 1\n",
    "                cc = ConflictChunk(cc['m_start'], cc['m_end'], cc['a_content'], cc['b_content'], cc['o_content'], cc['r_content'], cc['label'], cc['chunk_idx'])\n",
    "                cc_check(cc)\n",
    "    \n",
    "    def print_res_to_file(file=os.sys.stdout):\n",
    "        print(f'在 {dataset_name} 下统计结果:', file=file) \n",
    "        print(f'过滤过长后，共有 {resolvable_cc_cnt + non_resolvable_cc_cnt} 个冲突块，其中 {resolvable_cc_cnt} 个可以用编辑脚本解决，占比 {resolvable_cc_cnt / (resolvable_cc_cnt + non_resolvable_cc_cnt) * 100:.2f}%', file=file)\n",
    "        # print(f'有 {cc_with_es_intersects} 个冲突块的编辑脚本有交集', file=file)\n",
    "        print(f'有 {too_many_lines_cnt} 个冲突块的行数过大，无法处理', file=file)\n",
    "        print(f'有 {too_many_es_cnt} 个冲突块的编辑脚本数量过大，无法处理', file=file)\n",
    "        print(f'编辑脚本数量分布: {es_cnt}', file=file)\n",
    "        print(f'接受标记分布: {accept_mark_cnt}', file=file)\n",
    "        print(f'类型分布: {label_cnt}', file=file)\n",
    "        print(f'可解决类型分布: {label_resolvable_cnt}', file=file)\n",
    "        for k, v in label_cnt.items():\n",
    "            print(f'{k}: {v}, 可解决: {label_resolvable_cnt[k]}，占比: {label_resolvable_cnt[k] / v * 100:.2f}%', file=file)\n",
    "\n",
    "    # 新建文件夹\n",
    "    os.makedirs(work_dir / 'data_collect_analysis' / 'bt_log', exist_ok=True)\n",
    "    print_res_to_file(file=open(work_dir / 'data_collect_analysis' / 'bt_log' / f'{dataset_name}.log', 'w'))\n",
    "\n",
    "for dir2analyze in dirs:\n",
    "    analyze_edit_script(dir2analyze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 log 文件，绘制结果\n",
    "log_path = work_dir / \"data_collect_analysis\" / \"bt_log\" / \"100+stars_sample.log\"\n",
    "out_dir = work_dir / \"data_collect_analysis\" / \"bt_log\"\n",
    "# 类型分布: defaultdict(<class 'int'>, {'mixline': 4715, 'AB': 1485, 'B': 1590, 'A': 1969, 'newline': 3063, 'BA': 426, 'same modification, formatting maybe different': 68, 'O': 188})\n",
    "# 可解决类型分布: defaultdict(<class 'int'>, {'AB': 1382, 'B': 1558, 'A': 1932, 'mixline': 3452, 'BA': 391, 'same modification, formatting maybe different': 67, 'O': 188})\n",
    "dataset_name = log_path.stem.split('.')[0]\n",
    "\n",
    "# 输入类型分布和可解决类型分布，绘制柱状图\n",
    "def paint_bt_result(kind_counter, kind_resolvable, out_dir, dataset_name):\n",
    "    import plotly.graph_objects as go\n",
    "    fig = go.Figure()\n",
    "    labels = list(kind_counter.keys())\n",
    "    resolvable = [kind_resolvable[label] if label in kind_resolvable else 0 for label in labels]\n",
    "    non_resolvable = [kind_counter[label] - (kind_resolvable[label] if label in kind_resolvable else 0) for label in labels]\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=labels,\n",
    "        y=resolvable,\n",
    "        name='可解决'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=labels,\n",
    "        y=non_resolvable,\n",
    "        name='无法解决',\n",
    "        base=resolvable\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        barmode='stack',\n",
    "        title=f'{dataset_name} 回溯上界统计',\n",
    "        xaxis_title='冲突类型',\n",
    "        yaxis_title='数量',\n",
    "    )\n",
    "\n",
    "    # 保存为 html 文件\n",
    "    fig.write_html(out_dir / f'{dataset_name}_bt_result.html')\n",
    "\n",
    "def read_bt_log(log_path):\n",
    "    with open(log_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line.startswith('类型分布'):\n",
    "            kind_counter = eval(line.split(\"类型分布: defaultdict(<class 'int'>, \")[1][:-2])            # 从字符串中提取 dict\n",
    "        if line.startswith('可解决类型分布'):\n",
    "            kind_resolvable = eval(line.split(\"可解决类型分布: defaultdict(<class 'int'>, \")[1][:-2])\n",
    "    return kind_counter, kind_resolvable\n",
    "\n",
    "kind_counter, kind_resolvable = read_bt_log(log_path)\n",
    "paint_bt_result(kind_counter, kind_resolvable, out_dir, dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
